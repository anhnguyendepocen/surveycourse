\documentclass[12pt, a4]{article}
\usepackage[top=0cm, bottom=2cm, left=2cm, right=2cm]{geometry}
\usepackage{setspace}
\usepackage{mdwlist}
\usepackage{titlesec}

\title{Open Ended Responses\vspace{-2em}}
\author{}
\date{}

\begin{document}

\maketitle

\noindent The purpose of this activity is to think about how interviewers interpret and make sense of respondents' answers to questions; and how you as a researcher make sense of responses after the survey interview is already complete. On the course website, you will find two files containing responses to open-ended questions on the 2008 American National Election Studies. One file contains responses to a question asking respondents what they perceive as the most important problem facing the country. The other file contains responses to a question asking respondents to identify the office held by Gordon Brown (the UK prime minister at the time). Responses are recorded one per line as verbatim recordings written by interviewers during survey administration (duplicate responses have been deleted).

\subsection*{Most Important Problem}

\noindent For the most important problem (MIP) question, researchers are typically interested in systematically coding those responses into categories of issue concerns. For example, researchers may want to see how many individuals are concerned about the economy, compared to foreign policy, compared to other topics. The challenge, though, is translating responses into categories. This can be done ``in the field,'' where the interviewer tries to categorize a response into a fixed set of categories or, as was done on the ANES, by simply recording responses verbatim and coding them after the fact. Look through the set of verbatim responses to the MIP questions and think about how you would code these responses into issue categories. How easy is this after the fact? Would it be easier to conduct the coding in the field? Why? What kinds of responses are challenging to categorize?

\subsection*{Political Knowledge}

\noindent For knowledge questions, researchers are typically interested in coding whether a respondent had a ``correct'' or ``incorrect'' answer. This can be done ``in the field'' by having the interviewer determine whether an answer was correct or incorrect; or, as was done in the ANES, having the interviewer record the verbatim response and conduct the coding after the fact. Look through the set of verbatim responses to the knowledge questions and think about how you would categorize each response as correct or incorrect. What challenges do you face? Would it be easier to make judgments about correctness in the field or after all data are collected?

\subsection*{Interviewer Variance}

\noindent A major concern in interviewer-administered surveys is the notion of \textit{interviewer variance}. This is systematic variation in responses across different interviewers. For example, more experienced or less experienced interviewers may conduct interviews in different ways; interviewers of different ages, races, ethnicities, genders, etc. may increase or decrease certain types of socially desirability responding. Looking at the verbatim answers, how --- if at all --- might interviewer variance come in to play in handling verbatim responses? Think, for example, about how many times an interviewer might follow-up with a respondent about their answer (or their ``don't know'' response), about the amount of detail recorded in the verbatim recording, etc.

\end{document}
